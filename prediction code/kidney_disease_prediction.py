# -*- coding: utf-8 -*-
"""Kidney Disease Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XNcKswp1PN5wSBTJl6h1OKyT-AhNZCo6

# Chronic Kidney Disease - Prediction
"""

import pandas as pd
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
from pandas.plotting import scatter_matrix
data = pd.read_csv('/content/kidney_disease.csv').drop('id',axis=1)
data.head(5)

data.info()

data.dtypes

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
#converting categorial details to numericals
cols = ["rbc","pc","pcc","ba","pcv","wc","rc","htn","dm","cad","appet","pe","ane","classification"]

for col in cols:
    data[col] = le.fit_transform(data[col])
    print(le.classes_)

data.dtypes

data.isnull().sum()

data.head(5)

data.tail(5)

#Finding Null Values
for col in data.columns:
    data[col].fillna(data[col].median(),inplace=True)
data.head(50)

data.describe()

data.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt
data.hist(bins=50,figsize=(20,15))
plt.show()

correlation_figure, correlation_axis = plt.subplots(figsize = (30,25))
corr_mtrx = data.corr()
correlation_axis = sns.heatmap(corr_mtrx, annot= True)

plt.xticks(rotation = 30, horizontalalignment = 'right', fontsize = 20)
plt.yticks(fontsize = 20)
plt.show()

corr_matrix = data.corr()
corr_matrix['classification'].sort_values(ascending=False)

attributes = ['classification','hemo','sg','pcv','sod','pc','rc']
scatter_matrix(data[attributes],figsize=(12,8))

#Spliting entire data into Train and Test Segments
from numpy.random import RandomState
rng = RandomState()

train = data.sample(frac=0.7, random_state=rng)
test = data.loc[~data.index.isin(train.index)]
train.head(5)

test.head(5)

data = train[]
data.tail(50)

label = data['classification']
data = data.drop('classification',axis=1)
data.head(5)

test.to_csv('test.csv')

#Test and Train Split of Training Data
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(data,label,test_size=0.2, random_state=42)
def display_results(y_val,prediction):
    # Print the Confusion Matrix and slice it into four pieces
    cm = confusion_matrix(y_val,prediction)
    # visualize confusion matrix with seaborn heatmap
    cm_matrix = pd.DataFrame(data=cm)
    print("Model Accuracy:",accuracy_score(y_val,prediction))
    sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')

#Logistic Regression
from sklearn.linear_model import LogisticRegression,LinearRegression
from sklearn.metrics import accuracy_score, roc_curve, confusion_matrix, classification_report, roc_auc_score
clf = LogisticRegression(random_state=0, max_iter=1000).fit(X_train, y_train)
prediction = clf.predict(X_val)
print(classification_report(y_val,prediction))

display_results(y_val,prediction)

#tesing on test split
test_label = test['classification']
test_data = test[attributes].drop('classification',axis=1)
test_data.head(5)

predict = clf.predict(test_data)
display_results(test_label,predict)

#DECISION TREE
from sklearn.tree import DecisionTreeClassifier
# instantiate the DecisionTreeClassifier model with criterion gini index

clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)
# fit the model
clf_gini.fit(X_train, y_train)

prediction = clf_gini.predict(X_val)
print(classification_report(y_val,prediction))

plt.figure(figsize=(12,8))

from sklearn import tree

tree.plot_tree(clf_gini.fit(X_train, y_train))

display_results(y_val,prediction)

predict = clf_gini.predict(test_data)
display_results(test_label,predict)

#from sklearn.metrics import confusion_matrix, accuracy_score

# Commented out IPython magic to ensure Python compatibility.
#SVM
from sklearn.svm import LinearSVC,SVC
from sklearn.model_selection import GridSearchCV
# SVM classifier
svm = SVC(tol=1e-5)

# parameters
parameters = {
                'kernel': ['linea', 'poly', 'rbf', 'sigmoid'],
                'C': [0.01,0.03,0.1,0.3,1,3,10,30,100,300],
                'max_iter': [100,1000,5000,-1],
                'class_weight': [None, 'balanced'],
            }

# grid search for parameters
grid = GridSearchCV(estimator=svm, param_grid=parameters, n_jobs=-1)
grid.fit(X_train, y_train)
# print best scores
print("The best parameters are %s with a score of %0.4f\n"
#       % (grid.best_params_, grid.best_score_))

# prediction results
y_pred = grid.predict(X_val)
display_results(y_val,y_pred)

predict = grid.predict(test_data)
display_results(test_label,predict)

#KNN
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=2)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_val)

# print accuracy metrics
cm = confusion_matrix(y_val,y_pred)
print(accuracy_score(y_val,y_pred))
cm_matrix = pd.DataFrame(data=cm)

#sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')

predict = knn.predict(test_data)
display_results(test_label,predict)

#Random Forest
from sklearn.ensemble import RandomForestClassifier
rclf = RandomForestClassifier().fit(X_train, y_train)
prediction = clf.predict(X_val)
# print accuracy metrics
cm = confusion_matrix(y_val,prediction)
print(accuracy_score(y_val,prediction))

predict = rclf.predict(test_data)
display_results(test_label,predict)

#Adaboost
from sklearn.ensemble import AdaBoostClassifier
ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1),
    n_estimators=200,
    algorithm='SAMME.R',
    learning_rate = 0.5
)
ada_clf.fit(X_train, y_train)
prediction = ada_clf.predict(X_val)
# print accuracy metrics
cm = confusion_matrix(y_val,prediction)
print(accuracy_score(y_val,prediction))

predict = ada_clf.predict(test_data)
display_results(test_label,predict)

#All Model Accuracy with Visual------6 models
Algo=[clf,clf_gini,grid,knn,rclf,ada_clf]
Scores=[]
for K in Algo:
  K.fit(X_train,y_train)
  Scores.append(K.score(X_val,y_val)*100)
Algo=["Logistic regression","Decision tree","SVM","KNN","Random Forest","AdaBoost"]
Table=pd.DataFrame()
Table['Algorithms']=Algo
Table['Accuracy']=Scores
Table

# @title Algorithms vs Accuracy

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(Table['Algorithms'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(Table, x='Accuracy', y='Algorithms', inner='stick', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

sns.barplot(x='Algorithms',y='Accuracy',data=Table)



import pickle
filename = 'Kidney_RandomForest_model.pkl'
pickle.dump(rclf, open(filename, 'wb'))

import numpy as np

# Assuming your input data is stored in the variable 'data'
data = [13.1,1.025,33,135.0,1,26]

# Reshape the data into a 2D array with one column (-1 means infer the number of rows)
data_1d = le.fit_transform(data)

data_2d = data_1d.reshape(1, -1)

a = rclf.predict(data_2d)

print(a)

if a== 0:
    print ("NO")
elif a== 2 | 1:
    print ("Yes")