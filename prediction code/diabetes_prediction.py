# -*- coding: utf-8 -*-
"""Diabetes_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yeVeHbZLyCF80rjkaqCkO-61In5i_rO4
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from mlxtend.plotting import plot_confusion_matrix
from sklearn import tree
import math
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
warnings.filterwarnings('ignore')

diabetes = pd.read_csv("/content/diabetes_prediction_dataset.csv")

diabetes

print(diabetes.columns)

#for first 5 rows
diabetes.head()

#for last 5 rows
diabetes.tail()

#no of rows and columns
diabetes.shape

#getting info about data
diabetes.info()

diabetes.isnull().sum()

#statistical measures about the data
diabetes.describe()

#checking the distribution of target variable
diabetes['diabetes'].value_counts()

x=diabetes.drop(columns='diabetes',axis=1)
y=diabetes['diabetes']

print(y)

print(x)

# Checking duplicates
duplicate = diabetes[diabetes.duplicated()]
print("Duplicate Rows : ",len(duplicate))
duplicate

# Removing duplicate rows from the dataset
diabetes.drop_duplicates(inplace = True)

duplicate = diabetes[diabetes.duplicated()]
print("Duplicate Rows : ", len(duplicate))

# in our dataset the label is diabetes column
# This will return the label distribution count
diabetes['diabetes'].value_counts()

#correlation matrix
#it is to find the dependency of column to target value
corr = diabetes.corr()
plt.subplots(figsize=(12,10))
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(200, 20, as_cmap=True))

# plotting the label distribution
diabetes['diabetes'].value_counts().plot(kind = 'bar', title = 'Label Distribution')

X = diabetes.iloc[:,0:8] # features
y = diabetes[['diabetes']] # labels
# splitting the features and labels into train and test with test size = 20% and train size = 80%
X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=None)

X_train

# from sklearn.preprocessing import StandardScaler
# sc = StandardScaler()
# X_train= sc.fit_transform(X_train)
# X_test = sc.transform(X_test)



print("Shape of X_train",X_train.shape)
print("Shape of y_train",y_train.shape)
print("Shape of X_test",X_test.shape)
print("Shape of y_test",y_test.shape)

#random forest
model_1 = RandomForestClassifier(n_estimators = 300, criterion = 'entropy',
                             min_samples_split=10, random_state=0)
model_1.fit(X_train, y_train)
predictions = model_1.predict(X_test)
predictions

predictions

# Creating a new dataframe with true and predicted values
pred_df = pd.DataFrame()
pred_df['True values'] = y_test
pred_df['Predicted values'] = predictions
pred_df

# calculating the accuracy of the model
accuracies = {}
accuracy_1 = accuracy_score(y_test, predictions)
accuracies['Random Forest Classifier'] = accuracy_1
# calculating the classification report
classificationreport = classification_report(y_test, predictions)
# calculating the mse
mse = mean_squared_error(y_test, predictions)
# calculating the rmse
rmse = math.sqrt(mse)
print('\nAccuracy score of Random Forest Classifier : ' + str(round(accuracy_1*100, 2)))
print("\n"+"*"*50)
print('\nClassification_report : ')
print(classificationreport)
print("\n"+"*"*50)
print('\nMean squared error : '+ str(mse))
print("\n"+"*"*50)
print('\nRoot mean squared error : '+ str(rmse))

print('Confusion Matrix for Random Forest Classifier')
# calculating and plotting the confusion matrix
cm1 = confusion_matrix(y_test,predictions)
plot_confusion_matrix(conf_mat=cm1,show_absolute=True,
                                show_normed=True,
                                colorbar=True)
plt.show()

#logistic regression
model_2 = LogisticRegression()
model_2.fit(X_train, y_train)
predictions = model_2.predict(X_test)

# Creating a new dataframe with true and predicted values
pred_df = pd.DataFrame()
pred_df['True values'] = y_test
pred_df['Predicted values'] = predictions
pred_df

# calculating the accuracy of the model
accuracy_2 = accuracy_score(y_test, predictions)
accuracies['Logistic Regression'] = accuracy_2
# calculating the classification report
classificationreport = classification_report(y_test, predictions)
# calculating the mse
mse = mean_squared_error(y_test, predictions)
# calculating the rmse
rmse = math.sqrt(mse)
print('\nAccuracy score of Logistic Regression : ' + str(round(accuracy_2*100, 2)))
print("\n"+"*"*50)
print('\nClassification_report : ')
print(classificationreport)
print("\n"+"*"*50)
print('\nMean squared error : '+ str(mse))
print("\n"+"*"*50)
print('\nRoot mean squared error : '+ str(rmse))

print('Confusion Matrix for Logistic Regression')
# calculating and plotting the confusion matrix
cm1 = confusion_matrix(y_test,predictions)
plot_confusion_matrix(conf_mat=cm1,show_absolute=True,
                                show_normed=True,
                                colorbar=True)
plt.show()

#decision tree
model_3 = DecisionTreeClassifier(max_depth = 3)
model_3.fit(X_train, y_train)
predictions = model_3.predict(X_test)

# plotting decision tree classifier
plt.figure(figsize = (20,10))
tree.plot_tree(model_3)
plt.show()

# Creating a new dataframe with true and predicted values
pred_df = pd.DataFrame()
pred_df['True values'] = y_test
pred_df['Predicted values'] = predictions
pred_df

accuracy_3 = accuracy_score(y_test, predictions)
accuracies['Decision Tree Classifier'] = accuracy_3
classificationreport = classification_report(y_test, predictions)
mse = mean_squared_error(y_test, predictions)
rmse = math.sqrt(mse)
print('\nAccuracy score of Decision Tree Classifier : ' + str(round(accuracy_3*100, 2)))
print("\n"+"*"*50)
print('\nClassification_report : ')
print(classificationreport)
print("\n"+"*"*50)
print('\nMean squared error : '+ str(mse))
print("\n"+"*"*50)
print('\nRoot mean squared error : '+ str(rmse))

print('Confusion Matrix for Decision Tree Classifier')

# calculating and plotting the confusion matrix
cm1 = confusion_matrix(y_test,predictions)
plot_confusion_matrix(conf_mat=cm1,show_absolute=True,
                                show_normed=True,
                                colorbar=True)
plt.show()

model_4 = KNeighborsClassifier(n_neighbors = 5, p = 2)
model_4.fit(X_train, y_train)
predictions = model_4.predict(X_test)

# Creating a new dataframe with true and predicted values
pred_df = pd.DataFrame()
pred_df['True values'] = y_test
pred_df['Predicted values'] = predictions
pred_df

accuracy_4 = accuracy_score(y_test, predictions)
accuracies['KNeighborsClassifier'] = accuracy_4
classificationreport = classification_report(y_test, predictions)
mse = mean_squared_error(y_test, predictions)
rmse = math.sqrt(mse)
print('\nAccuracy score of K - Nearest Neighbors : ' + str(round(accuracy_4*100, 2)))
print("\n"+"*"*50)
print('\nClassification_report : ')
print(classificationreport)
print("\n"+"*"*50)
print('\nMean squared error : '+ str(mse))
print("\n"+"*"*50)
print('\nRoot mean squared error : '+ str(rmse))

print('Confusion Matrix for K - Nearest Neighbors')
# calculating and plotting the confusion matrix
cm1 = confusion_matrix(y_test,predictions)
plot_confusion_matrix(conf_mat=cm1,show_absolute=True,
                                show_normed=True,
                                colorbar=True)
plt.show()

colors = ["purple", "green", "orange", "magenta"]
plt.figure(figsize = (15,6))
plt.grid(True)
sns.set_style("whitegrid")
sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)
plt.title('Comparing accuracy of the four models', fontsize = 20)
plt.xticks(fontsize = 12)
plt.yticks(fontsize = 12)
plt.ylabel("Accuracy %")
plt.xlabel("Algorithms")


plt.show()

df_accuracy = pd.DataFrame(accuracies.items(), columns=['Model', 'Accuracy'])
df_accuracy.sort_values(by= ['Accuracy'], inplace= True, ascending= False)
df_accuracy

# @title Model vs Accuracy

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(df_accuracy['Model'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(df_accuracy, x='Accuracy', y='Model', inner='stick', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

# Creating a pickle file for the classifier
import pickle
filename = 'Decision_tree_model.pkl'
pickle.dump(model_3, open(filename, 'wb'))

import numpy as np

# Assuming your input data is stored in the variable 'data'
data = np.array([1,53,0,0,3,27.32,7,159])

# Reshape the data into a 2D array with one column (-1 means infer the number of rows)
data_2d = data.reshape(1, -1)

a = model_3.predict(data_2d)

print(a)

if a==0:
    print ("NO")
elif a==1:
    print ("Yes")

