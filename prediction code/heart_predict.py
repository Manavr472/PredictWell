# -*- coding: utf-8 -*-
"""heart_predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O-gjk0NckUPtx_AY6DYwpqNcCXh01DWT
"""

#Importing libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report,accuracy_score,confusion_matrix
from sklearn.metrics import f1_score,precision_score, recall_score

#Load dataset
df = pd.read_csv("/content/heart.csv")

type(df)

#no of rows and columns
df.shape

#getting info about data
df.info()

#for first 5 rows
df.head()

#for last 5 rows
df.tail()

##feature engineering
#To get null values of column
df.isnull().sum()

#heatmap to show null values
sns.heatmap(df.isnull())
#as dataset don't have any null value so it is showing the red colour which is 0.0 according to scale

df.dtypes

df['Heart Disease'].value_counts()

#To show the presence and absence of Heart disease using pie chart
plt.pie(x = df['Heart Disease'].value_counts() ,autopct='%1.3f%%' ,labels=['No', 'Yes'], colors=['blue', 'red'], shadow=True)
plt.title('Ratio of Heart Disease')
plt.show()

#correlation matrix
#it is to find the dependency of column to target value
corr = df.corr()
plt.subplots(figsize=(12,10))
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(200, 20, as_cmap=True))

#visualisation of relationships between variables within a dataset
# sns.pairplot(df.drop(columns='Heart Disease') , height=3,aspect=1);

#to visualize the feature using histogram
df.hist(figsize=(12,12))

#Line graph for heart disease vs age
sns.lineplot(data=df,x='Age',y='Heart Disease')

#count graph for heart disease vs Thallium
plt.figure(figsize=(20,20))
sns.countplot(data=df,hue='Heart Disease',x='Thallium')

#statistical measures about the data
df.describe()

#Training and testing

X = df.iloc[:, :-1]
y = df["Heart Disease"]

#Spliting the data
X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=None)

# features scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train=sc.fit_transform(X_train)
X_test = sc.transform(X_test)

print("Shape of X_train",X_train.shape)
print("Shape of y_train",y_train.shape)
print("Shape of X_test",X_test.shape)
print("Shape of y_test",y_test.shape)

# Encoding the target value into numerical
# value using LabelEncoder
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
df["Heart Disease"] = encoder.fit_transform(df["Heart Disease"])

df["Heart Disease"]

#Decision Tree
print("Decision Tree")
from sklearn.tree import DecisionTreeClassifier
tree=DecisionTreeClassifier( max_depth=5)
# tree.fit(X_train, y_train)
# pred=tree.predict(X_test)
# acc=accuracy_score(y_test,pred)
# #print("Acurray on test set: {:.2f}%".format(acc*100))
# print("ACCURACY score on train data",accuracy_score(y_train, tree.predict(X_train))*100)
# print("Accuracy Score on test data", accuracy_score(y_test,pred)*100)
# print("Classification Report\n",classification_report(y_test,pred),"\n")
# print("Confusion Matrix\n",confusion_matrix(y_test,pred))
# sns.heatmap(confusion_matrix(y_test,pred),annot=True,fmt='d')

#to find the best score of KNN - to get best value of k
from sklearn.neighbors import KNeighborsClassifier
# knn_scores=[]
# for k in range(1,40):
#   knnn=KNeighborsClassifier(n_neighbors=k)
#   knnn.fit(X_train,y_train)
#   knn_scores.append(knnn.score(X_test,y_test))

# print(f'Best choice of k: {np.argmax(knn_scores)+1}')

#KNN
print("KNN")
from sklearn.neighbors import KNeighborsClassifier
KNN=KNeighborsClassifier(n_neighbors=12)
# KNN.fit(X_train,y_train)
# pred=KNN.predict(X_test)
# print("ACCURACY score on train data",accuracy_score(y_train, KNN.predict(X_train))*100)
# print("Accuracy Score on test data", accuracy_score(y_test,pred)*100)
# print("Classification Report\n",classification_report(y_test,pred),"\n")
# print("Confusion Matrix\n",confusion_matrix(y_test,pred))
# sns.heatmap(confusion_matrix(y_test,pred),annot=True,fmt='d')

##Naive Bayes
print("Naive Bayes")
from sklearn.naive_bayes import GaussianNB
nb_model = GaussianNB()
# nb_model.fit(X_train, y_train)
# pred = nb_model.predict(X_test)
# print("ACCURACY score on train data",accuracy_score(y_train, nb_model.predict(X_train))*100)
# print("ACCURACY on test data",accuracy_score(y_test, pred)*100)
# print("Classification Report\n",classification_report(y_test,pred),"\n")
# print("Confusion Matrix",confusion_matrix(y_test,pred))
# sns.heatmap(confusion_matrix(y_test,pred),annot=True,fmt='d')

#Logistic Regression
print("Logistic Regression")
from sklearn.linear_model import LogisticRegression
lr_model=LogisticRegression()
#training the logistic regression model with training data
# lr_model.fit(X_train,y_train)
# pred = lr_model.predict(X_test)
# print("ACCURACY score on train data",accuracy_score(y_train, lr_model.predict(X_train))*100)
# print("Accuracy Score on test data", accuracy_score(y_test,pred)*100)
# print("Classification Report\n",classification_report(y_test,pred),"\n")
# print("Confusion Matrix",confusion_matrix(y_test,pred))
# sns.heatmap(confusion_matrix(y_test,pred),annot=True,fmt='d')

#to get best n_estimators for random forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
rf_g={
    'n_estimators': range(1,1000,100),
}
rf=RandomForestClassifier()
rf_ran = RandomizedSearchCV(param_distributions=rf_g,estimator=rf,scoring="accuracy",verbose=0,n_iter=100,cv=4)
# rf_ran.fit(X_train,y_train)
# best_params=rf_ran.best_params_
# print(best_params)

#Random Forest classifier
print("Random Forest Classification")
from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators = 401, criterion = 'entropy', random_state = 101,max_depth=4)
# rf_classifier.fit(X_train, y_train)
# pred = rf_classifier.predict(X_test)
# print("ACCURACY score on train data",accuracy_score(y_train, rf_classifier.predict(X_train))*100)
# print("Accuracy Score on test data", accuracy_score(y_test,pred)*100)
# print("Classification Report\n",classification_report(y_test,pred),"\n")
# print("Confusion Matrix",confusion_matrix(y_test,pred))
# sns.heatmap(confusion_matrix(y_test,pred),annot=True,fmt='d')

#to get best kernel for svm
from sklearn import svm
svc_scores = []
kernels = ['linear', 'poly', 'rbf', 'sigmoid']
for i in range(len(kernels)):
    svc_classifier = svm.SVC(kernel = kernels[i])
    svc_classifier.fit(X_train, y_train)
    svc_scores.append(svc_classifier.score(X_test, y_test))

print(kernels, svc_scores)

#SVM
print("SVM")
from sklearn import svm
sup = svm.SVC(kernel='sigmoid')
# sup.fit(X_train,y_train)
# pred = sup.predict(X_test)
# print("ACCURACY score on train data",accuracy_score(y_train, sup.predict(X_train))*100)
# print("Accuracy Score on test data", accuracy_score(y_test,pred)*100)
# print("Classification Report\n",classification_report(y_test,pred),"\n")
# print("Confusion Matrix",confusion_matrix(y_test,pred))
# sns.heatmap(confusion_matrix(y_test,pred),annot=True,fmt='d')

#All Model Accuracy with Visual------6 models
Algo=[tree,KNN,nb_model,lr_model,rf_classifier,sup]
Scores=[]
for K in Algo:
  K.fit(X_train,y_train)
  Scores.append(K.score(X_test,y_test)*100)
Algo=["Decision Tree","K Neighbors","Naive Bayes","Logistic Regression","Random Forest","SVM"]
Table=pd.DataFrame()
Table['Algorithms']=Algo
Table['Accuracy']=Scores
Table

# @title Algorithms vs Accuracy

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(Table['Algorithms'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(Table, x='Accuracy', y='Algorithms', inner='stick', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

#Graph to show accuracy of the models
sns.barplot(x='Algorithms',y='Accuracy',data=Table)

a=KNN.predict(sc.transform([[70,0,0,110,230,1,1,140,0,3.2,0,0,0]]))
if a==1:
    print ("presence of heart disease")

else:
    print ("absence of heart disease")

# Creating a pickle file for the classifier
import pickle
filename = 'Heart_KNN_model.pkl'
pickle.dump(KNN, open(filename, 'wb'))